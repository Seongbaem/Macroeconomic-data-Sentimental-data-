{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#날짜 시간 데이터 입력\n",
    "\n",
    "# start = 1\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "while True:\n",
    "    # try:\n",
    "        윤년 = [ i for i in range(2004, 2021, 4)]\n",
    "        평년 = [ i for i in range(2003,2022,1)]\n",
    "        day_1 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30']\n",
    "        day_2 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31']\n",
    "        day_3 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28'] \n",
    "        day_4 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29'] \n",
    "        # start = 1\n",
    "        k = [ i for i in range(1,212,10)] # 1페이지부터 13페이지 \n",
    "        for year in 평년:\n",
    "            for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:                        \n",
    "              if month in ['04','06','09','11']: # 1달에 30일 \n",
    "                for day in day_1:\n",
    "                  for start in k: #k는 페이지 넘버\n",
    "                    url = 'https://search.naver.com/search.naver?where=news&query=삼성전기&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds='+ str(year) + '.' + str(month) + '.' + str(day) + '&de=' + str(year) +'.' + str(month) + '.' + str(day)+'&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom'+ str(year) + str(month) + str(day) + 'to' + str(year) + str(month) + str(day)+',a:all&start=' + str(start)\n",
    "                    \n",
    "                    # start += 10\n",
    "                    print(url)\n",
    "                    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "                      'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                    #print(response)\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    #print(soup)\n",
    "\n",
    "\n",
    "                    news_title = [title['title'] for title in soup.find_all('a', attrs={'class':'news_tit'}) if '삼성전기' in title['title']]   # 기사 제목\n",
    "                    news_url = [ url['href'] for url in soup.find_all('a', attrs={'class':'news_tit'}) ] # 기사 url\n",
    "\n",
    "                    # dates = [ date.get_text() for date in soup.find_all('span', attrs={'class':'info'})] # 기사 작성일\n",
    "                    news_date = []\n",
    "                    # for date in dates:\n",
    "                    #     if re.search(r'\\d+.\\d+.\\d+.', date) != None: # 기사 작성일 정제\n",
    "                    #         news_date.append(date)\n",
    "\n",
    "                    for i in range(len(news_title)):\n",
    "                        news_date.append(str(year)+'.'+str(month)+'.'+str(day))\n",
    "                    # print(news_date)\n",
    "\n",
    "                    df = pd.DataFrame({'기사작성일':news_date,'기사제목':news_title})\n",
    "                    result_df = pd.concat([result_df, df], ignore_index=True)\n",
    "\n",
    "                    print(df)\n",
    "#                     if len(news_title) >= 5:\n",
    "#                       break\n",
    "\n",
    "              elif month in ['01','03','05','07','08','10','12']: # 1달에 31일 \n",
    "                for day in day_2:\n",
    "                  for start in k: #k는 페이지 넘버\n",
    "                    url = 'https://search.naver.com/search.naver?where=news&query=삼성전기&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=' + str(year) + '.' + str(month) + '.' + str(day) + '&de=' + str(year) +'.' + str(month) + '.' + str(day)+'&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom' + str(year) + str(month) + str(day) + 'to' + str(year) + str(month) + str(day)+',a:all&start=' + str(start)\n",
    "                    \n",
    "                    # start += 10\n",
    "                    print(url)\n",
    "                    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "                      'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                    #print(response)\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    #print(soup)\n",
    "\n",
    "\n",
    "                    news_title = [title['title'] for title in soup.find_all('a', attrs={'class':'news_tit'}) if '삼성전기' in title['title']]   # 기사 제목\n",
    "                    news_url = [ url['href'] for url in soup.find_all('a', attrs={'class':'news_tit'}) ] # 기사 url\n",
    "\n",
    "                    # dates = [ date.get_text() for date in soup.find_all('span', attrs={'class':'info'})] # 기사 작성일\n",
    "                    news_date = []\n",
    "                    # for date in dates:\n",
    "                    #     if re.search(r'\\d+.\\d+.\\d+.', date) != None: # 기사 작성일 정제\n",
    "                    #         news_date.append(date)\n",
    "\n",
    "                    for i in range(len(news_title)):\n",
    "                        news_date.append(str(year)+'.'+str(month)+'.'+str(day))\n",
    "                    # print(news_date)\n",
    "\n",
    "                    df = pd.DataFrame({'기사작성일':news_date,'기사제목':news_title})\n",
    "                    result_df = pd.concat([result_df, df], ignore_index=True)\n",
    "\n",
    "                    print(df)\n",
    "#                     if len(news_title) >= 5:\n",
    "#                       break\n",
    "              elif month in ['02']: # 1달에 31일 \n",
    "                if year in 윤년:\n",
    "                    for day in day_4:\n",
    "                      for start in k: #k는 페이지 넘버\n",
    "                        url = 'https://search.naver.com/search.naver?where=news&query=삼성전기&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=' + str(year) + '.' + str(month) + '.' + str(day) + '&de=' + str(year) +'.' + str(month) + '.' + str(day)+'&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom' + str(year) + str(month) + str(day) + 'to' + str(year) + str(month) + str(day)+',a:all&start=' + str(start)\n",
    "\n",
    "                        # start += 10\n",
    "                        print(url)\n",
    "                        headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "                          'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "                        response = requests.get(url, headers=headers)\n",
    "                        #print(response)\n",
    "                        soup = BeautifulSoup(response.text, 'lxml')\n",
    "                        #print(soup)\n",
    "\n",
    "\n",
    "                        news_title = [title['title'] for title in soup.find_all('a', attrs={'class':'news_tit'}) if '삼성전기' in title['title']]   # 기사 제목\n",
    "                        news_url = [ url['href'] for url in soup.find_all('a', attrs={'class':'news_tit'}) ] # 기사 url\n",
    "\n",
    "                        # dates = [ date.get_text() for date in soup.find_all('span', attrs={'class':'info'})] # 기사 작성일\n",
    "                        news_date = []\n",
    "                        # for date in dates:\n",
    "                        #     if re.search(r'\\d+.\\d+.\\d+.', date) != None: # 기사 작성일 정제\n",
    "                        #         news_date.append(date)\n",
    "\n",
    "                        for i in range(len(news_title)):\n",
    "                            news_date.append(str(year)+'.'+str(month)+'.'+str(day))\n",
    "                        # print(news_date)\n",
    "\n",
    "                        df = pd.DataFrame({'기사작성일':news_date,'기사제목':news_title})\n",
    "                        result_df = pd.concat([result_df, df], ignore_index=True)\n",
    "\n",
    "                        print(df)\n",
    "                        \n",
    "                elif year not in 윤년:\n",
    "                    for day in day_3:\n",
    "                      for start in k: #k는 페이지 넘버\n",
    "                        url = 'https://search.naver.com/search.naver?where=news&query=삼성전기&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=' + str(year) + '.' + str(month) + '.' + str(day) + '&de=' + str(year) +'.' + str(month) + '.' + str(day)+'&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom' + str(year) + str(month) + str(day) + 'to' + str(year) + str(month) + str(day)+',a:all&start=' + str(start)\n",
    "\n",
    "                        # start += 10\n",
    "                        print(url)\n",
    "                        headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "                          'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "                        response = requests.get(url, headers=headers)\n",
    "                        #print(response)\n",
    "                        soup = BeautifulSoup(response.text, 'lxml')\n",
    "                        #print(soup)\n",
    "\n",
    "\n",
    "                        news_title = [title['title'] for title in soup.find_all('a', attrs={'class':'news_tit'}) if '삼성전기' in title['title']]   # 기사 제목\n",
    "                        news_url = [ url['href'] for url in soup.find_all('a', attrs={'class':'news_tit'}) ] # 기사 url\n",
    "\n",
    "                        # dates = [ date.get_text() for date in soup.find_all('span', attrs={'class':'info'})] # 기사 작성일\n",
    "                        news_date = []\n",
    "                        # for date in dates:\n",
    "                        #     if re.search(r'\\d+.\\d+.\\d+.', date) != None: # 기사 작성일 정제\n",
    "                        #         news_date.append(date)\n",
    "\n",
    "                        for i in range(len(news_title)):\n",
    "                            news_date.append(str(year)+'.'+str(month)+'.'+str(day))\n",
    "                        # print(news_date)\n",
    "\n",
    "                        df = pd.DataFrame({'기사작성일':news_date,'기사제목':news_title})\n",
    "                        result_df = pd.concat([result_df, df], ignore_index=True)\n",
    "\n",
    "                        print(df)\n",
    "    #                     if len(news_title) >= 5:\n",
    "#                       break\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "  \n",
    "    # except: # 오류발생시 몇 페이지까지 크롤링했는지 page를 확인하기 \n",
    "    #     print(start)\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
